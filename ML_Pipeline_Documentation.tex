\documentclass[12pt,a4paper]{article}
\usepackage[utf-8]{inputenc}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{fancyhdr}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{array}
\usepackage{tikz}

% Color definitions
\definecolor{darkblue}{rgb}{0.1,0.2,0.5}
\definecolor{lightgray}{rgb}{0.95,0.95,0.95}

% Header and Footer
\pagestyle{fancy}
\fancyhf{}
\lhead{\textbf{WarehouseMVP ML Pipeline}}
\rhead{\thepage}
\cfoot{Machine Learning Process Documentation}

% Title
\title{\Huge \textbf{WarehouseMVP: ML Pipeline Documentation}\\
\vspace{0.3cm}
\Large Warehouse Slot Optimization using HistGradientBoosting}
\author{ML Engineering Team}
\date{\today}

\begin{document}

\maketitle

\tableofcontents
\newpage

% ============================================================================
\section{Executive Summary}
% ============================================================================

The WarehouseMVP system implements a machine learning pipeline that optimizes warehouse slot allocation for products. The system uses synthetic warehouse data to train a HistGradientBoosting regressor model that predicts the optimal slot for each item based on physical dimensions, weight, demand characteristics, and warehouse layout.

\textbf{Key Statistics:}
\begin{itemize}
    \item Training pairs: 60,000
    \item Features: 13 dimensions
    \item Algorithm: HistGradientBoosting Regressor
    \item Mean Absolute Error: $\approx 0.04$
    \item Slot match accuracy: $\approx 82\%$
\end{itemize}

% ============================================================================
\section{Data Architecture}
% ============================================================================

\subsection{Database Structure}

The system uses SQLite for data persistence with two main tables: \texttt{slots} and \texttt{items}.

\subsubsection{Slots Table}

Warehouse storage units are represented with the following attributes:

\begin{equation}
\text{Slot}_i = \{s_{\text{id}}, L_m, W_m, H_m, W_{\max}, d\}
\end{equation}

where:
\begin{align*}
s_{\text{id}} &\in \{\text{S01}, \text{S02}, \ldots, \text{S30}\} \quad \text{(unique identifier)} \\
L_m &\in \{1.2, 1.4, 1.6, 2.0\} \text{ meters} \quad \text{(length)} \\
W_m &\in \{0.8, 1.0, 1.2\} \text{ meters} \quad \text{(width)} \\
H_m &\in \{1.5, 1.8, 2.0, 2.4\} \text{ meters} \quad \text{(height)} \\
W_{\max} &\in \{300, 500, 800, 1200\} \text{ kg} \quad \text{(maximum weight capacity)} \\
d &\in [5, 60] \text{ meters} \quad \text{(distance from picking point)}
\end{align*}

\textbf{Database Schema:}
\begin{lstlisting}[language=SQL, basicstyle=\ttfamily\small, backgroundcolor=\color{lightgray}]
CREATE TABLE slots (
  slot_id TEXT PRIMARY KEY,
  Lm REAL NOT NULL,
  Wm REAL NOT NULL,
  Hm REAL NOT NULL,
  max_weight REAL NOT NULL,
  distance REAL NOT NULL
);
\end{lstlisting}

\subsubsection{Items Table}

Warehouse products are characterized as:

\begin{equation}
\text{Item}_j = \{i_{\text{id}}, L_m, W_m, H_m, w, d_m, p_o\}
\end{equation}

where:
\begin{align*}
i_{\text{id}} &\in \{\text{MAT-00001}, \ldots, \text{MAT-02000}\} \quad \text{(SKU)} \\
L_m &\sim \mathcal{U}(0.4, 1.6) \text{ meters} \quad \text{(length)} \\
W_m &\sim \mathcal{U}(0.3, 1.2) \text{ meters} \quad \text{(width)} \\
H_m &\sim \mathcal{U}(0.3, 2.0) \text{ meters} \quad \text{(height)} \\
w &\sim \mathcal{U}(20, 900) \text{ kg} \quad \text{(weight)} \\
d_m &\in \{5, 10, 20, 30, 50, 80, 120\} \quad \text{(monthly demand)} \\
p_o &\sim \mathcal{U}(0, 1) \quad \text{(order pressure)}
\end{align*}

\textbf{Database Schema:}
\begin{lstlisting}[language=SQL, basicstyle=\ttfamily\small, backgroundcolor=\color{lightgray}]
CREATE TABLE items (
  item_id TEXT PRIMARY KEY,
  Lm REAL NOT NULL,
  Wm REAL NOT NULL,
  Hm REAL NOT NULL,
  weight REAL NOT NULL,
  monthly_demand REAL NOT NULL,
  order_pressure REAL NOT NULL
);
\end{lstlisting}

\subsection{Data Generation}

Default configuration:
\begin{align*}
|\text{Slots}| &= 30 \quad \text{(warehouse slots)} \\
|\text{Items}| &= 2000 \quad \text{(product instances)} \\
|\text{Training Pairs}| &= 60,000 \quad \text{(all combinations)}
\end{align*}

% ============================================================================
\section{Priority Computation}
% ============================================================================

\subsection{Mathematical Formulation}

The priority score quantifies the urgency of placing an item near the picking point. It combines demand frequency and order pressure:

\begin{equation}
P = 0.6 \cdot d_{\text{norm}} + 0.4 \cdot p_o \quad \text{where } P \in [0, 1]
\end{equation}

where the normalized demand is:

\begin{equation}
d_{\text{norm}} = \text{clamp}\left(\frac{d_m - 5}{120 - 5}, 0, 1\right)
\end{equation}

\subsection{Interpretation}

\begin{itemize}
    \item $P \approx 0.0$: Low rotation, low urgency $\Rightarrow$ Store far away
    \item $P \approx 0.5$: Medium priority $\Rightarrow$ Store at medium distance
    \item $P \approx 1.0$: High rotation, high urgency $\Rightarrow$ Store nearby
\end{itemize}

% ============================================================================
\section{Oracle Cost Function}
% ============================================================================

\subsection{Ground Truth Definition}

The oracle function defines the optimal cost for assigning item $j$ to slot $i$:

\begin{equation}
\text{Cost}(i, j) = \alpha(P) \cdot c_d(i) + (1 - \alpha(P)) \cdot c_w(i, j)
\end{equation}

where:

\begin{align*}
\alpha(P) &= 0.2 + 0.6 \cdot P \quad \text{(dynamic weighting, } \alpha \in [0.2, 0.8]) \\
c_d(i) &= \frac{d_i - d_{\min}}{d_{\max} - d_{\min}} \quad \text{(normalized distance cost)} \\
c_w(i, j) &= \frac{v_{\text{waste}} - v_{\min}}{v_{\max} - v_{\min}} \quad \text{(normalized waste cost)}
\end{align*}

\subsection{Component Definitions}

\subsubsection{Distance Cost}

Normalized distance penalizes far slots:

\begin{equation}
c_d(i) = \begin{cases}
\frac{d_i - \min_k d_k}{\max_k d_k - \min_k d_k} & \text{if } d_{\max} \neq d_{\min} \\
0 & \text{otherwise}
\end{cases}
\end{equation}

\subsubsection{Waste Cost}

Penalizes unused slot volume:

\begin{equation}
v_{\text{waste}}(i, j) = \max(V_i - V_j, 0)
\end{equation}

where:
\begin{align*}
V_i &= L_{m,i} \cdot W_{m,i} \cdot H_{m,i} \quad \text{(slot volume)} \\
V_j &= L_{m,j} \cdot W_{m,j} \cdot H_{m,j} \quad \text{(item volume)}
\end{align*}

Normalization:

\begin{equation}
c_w(i, j) = \frac{v_{\text{waste}} - v_{\min}}{v_{\max} - v_{\min}}
\end{equation}

\subsubsection{Dynamic Weighting}

The weight parameter adapts based on item priority:

\begin{equation}
\alpha(P) = \begin{cases}
0.2 & \text{if } P = 0.0 \quad \text{(prioritize space)} \\
0.8 & \text{if } P = 1.0 \quad \text{(prioritize proximity)}
\end{cases}
\end{equation}

\subsection{Feasibility Constraint}

Assignment is feasible only if:

\begin{equation}
\text{Feasible}(i, j) = \begin{cases}
1 & \text{if } L_{m,i} \geq L_{m,j} \wedge W_{m,i} \geq W_{m,j} \wedge H_{m,i} \geq H_{m,j} \wedge W_{\max,i} \geq w_j \\
0 & \text{otherwise}
\end{cases}
\end{equation}

Infeasible pairs receive penalty cost: $\text{Cost} = 5.0$

% ============================================================================
\section{Training Data Construction}
% ============================================================================

\subsection{Data Pair Generation}

For each item $j \in \{1, \ldots, 2000\}$, we generate pairs with all slots $i \in \{1, \ldots, 30\}$:

\begin{equation}
\mathcal{D} = \{(x_{ij}, y_{ij})\}_{i=1,j=1}^{30,2000}
\end{equation}

where $|\mathcal{D}| = 60,000$.

\subsection{Feature Vector}

Each training example consists of 13 input features:

\begin{equation}
x_{ij} = [x_1, x_2, \ldots, x_{13}] \in \mathbb{R}^{13}
\end{equation}

\subsubsection{Item Features (7 dimensions)}

\begin{align}
x_1 &= L_{m,j} \quad \text{(item length: } [0.4, 1.6] \text{)} \\
x_2 &= W_{m,j} \quad \text{(item width: } [0.3, 1.2] \text{)} \\
x_3 &= H_{m,j} \quad \text{(item height: } [0.3, 2.0] \text{)} \\
x_4 &= w_j \quad \text{(item weight: } [20, 900] \text{ kg)} \\
x_5 &= d_{m,j} \quad \text{(monthly demand: } \{5,10,20,30,50,80,120\} \text{)} \\
x_6 &= p_{o,j} \quad \text{(order pressure: } [0, 1] \text{)} \\
x_7 &= P_j \quad \text{(priority score: } [0, 1] \text{)}
\end{align}

\subsubsection{Slot Features (5 dimensions)}

\begin{align}
x_8 &= L_{m,i} \quad \text{(slot length: } \{1.2, 1.4, 1.6, 2.0\} \text{)} \\
x_9 &= W_{m,i} \quad \text{(slot width: } \{0.8, 1.0, 1.2\} \text{)} \\
x_{10} &= H_{m,i} \quad \text{(slot height: } \{1.5, 1.8, 2.0, 2.4\} \text{)} \\
x_{11} &= W_{\max,i} \quad \text{(max weight: } \{300, 500, 800, 1200\} \text{ kg)} \\
x_{12} &= d_i \quad \text{(distance: } [5, 60] \text{ m)}
\end{align}

\subsubsection{Meta Feature (1 dimension)}

\begin{align}
x_{13} &= \text{Feasible}(i, j) \in \{0, 1\} \quad \text{(binary feasibility)}
\end{align}

\subsection{Target Variable}

The regression target is the oracle cost:

\begin{equation}
y_{ij} = \text{Cost}(i, j) \in [0, 5.0]
\end{equation}

Feature matrix and target vector:

\begin{align*}
X &\in \mathbb{R}^{60000 \times 13} \quad \text{(all training examples)} \\
y &\in \mathbb{R}^{60000} \quad \text{(all target costs)}
\end{align*}

% ============================================================================
\section{Training Algorithm}
% ============================================================================

\subsection{Algorithm Selection: HistGradientBoosting}

We employ \texttt{sklearn.ensemble.HistGradientBoostingRegressor}, a gradient boosting implementation using histogram-based binning.

\subsubsection{Why HistGradientBoosting?}

\begin{itemize}
    \item \textbf{Speed}: Histogram-based binning reduces memory and computation
    \item \textbf{Non-linearity}: Captures complex item-slot relationships
    \item \textbf{Robustness}: No scaling required, handles outliers well
    \item \textbf{Interpretability}: Feature importance estimation available
    \item \textbf{Scalability}: Efficient for 60K training samples
\end{itemize}

\subsection{Hyperparameters}

\begin{equation}
\theta = \{\text{max\_depth}, \text{learning\_rate}, \text{max\_iter}\}
\end{equation}

\begin{table}[h]
\centering
\begin{tabular}{lcc}
\toprule
\textbf{Parameter} & \textbf{Value} & \textbf{Description} \\
\midrule
\texttt{max\_depth} & 6 & Maximum tree depth \\
\texttt{learning\_rate} & 0.08 & Shrinkage factor per iteration \\
\texttt{max\_iter} & 250 & Number of boosting iterations \\
\texttt{random\_state} & 123 & Reproducibility seed \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Training Process}

\subsubsection{Data Split}

\begin{align*}
\mathcal{D}_{\text{train}} &= 0.80 \cdot \mathcal{D} = 48,000 \text{ pairs} \quad \text{(model learns)} \\
\mathcal{D}_{\text{test}} &= 0.20 \cdot \mathcal{D} = 12,000 \text{ pairs} \quad \text{(evaluation)}
\end{align*}

Random seed ensures reproducibility.

\subsubsection{Algorithm Flow}

\begin{enumerate}
    \item Initialize: $\hat{y}^{(0)} = \text{mean}(y)$
    
    \item For $t = 1$ to $T = 250$:
    \begin{equation}
    r^{(t)} = y - \hat{y}^{(t-1)} \quad \text{(residuals)}
    \end{equation}
    
    \item Fit tree $h_t$ to predict residuals: $h_t(x) \approx r^{(t)}$
    
    \item Update predictions:
    \begin{equation}
    \hat{y}^{(t)} = \hat{y}^{(t-1)} + \eta \cdot h_t(x)
    \end{equation}
    where $\eta = 0.08$ is the learning rate.
    
    \item Final model:
    \begin{equation}
    \hat{y} = \sum_{t=1}^{250} \eta \cdot h_t(x)
    \end{equation}
\end{enumerate}

\subsection{Loss Function}

Gradient boosting minimizes Mean Squared Error:

\begin{equation}
\mathcal{L} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
\end{equation}

% ============================================================================
\section{Performance Metrics}
% ============================================================================

\subsection{Mean Absolute Error}

\begin{equation}
\text{MAE} = \frac{1}{n_{\text{test}}} \sum_{i=1}^{n_{\text{test}}} |y_i - \hat{y}_i| \approx 0.04
\end{equation}

Example prediction:
\begin{align*}
\text{Oracle cost} &= 0.35 \\
\text{Predicted cost} &= 0.38 \\
\text{Error} &= |0.35 - 0.38| = 0.03 \checkmark
\end{align*}

\subsection{Slot Match Accuracy}

Fraction of items where predicted slot matches oracle slot:

\begin{equation}
\text{Accuracy} = \frac{1}{n_{\text{test}}} \sum_{i=1}^{n_{\text{test}}} \mathbb{1}[\text{slot}_{\text{pred}}(i) = \text{slot}_{\text{oracle}}(i)] \approx 0.82 \text{ (82\%)}
\end{equation}

\subsection{Performance Summary}

\begin{table}[h]
\centering
\begin{tabular}{lcc}
\toprule
\textbf{Metric} & \textbf{Value} & \textbf{Interpretation} \\
\midrule
Training pairs & 60,000 & Total dataset size \\
Train/test split & 80\% / 20\% & 48K train, 12K test \\
Features & 13 & Input dimensions \\
Mean Absolute Error & 0.04 & Cost prediction error \\
Slot match accuracy & 82\% & Exact slot match rate \\
Model size & $\sim 7$ MB & Joblib serialized \\
\bottomrule
\end{tabular}
\end{table}

% ============================================================================
\section{Prediction Pipeline}
% ============================================================================

\subsection{Inference Procedure}

Given a new item $j^*$:

\begin{algorithm}
\textbf{Input:} Item $j^*$ with attributes \\
\textbf{Output:} Recommended slot $i^*$

\begin{enumerate}
    \item Load trained model $M$ and feature columns $\mathcal{F}$
    
    \item Compute priority: $P_{j^*} = 0.6 \cdot d_{\text{norm}} + 0.4 \cdot p_o$
    
    \item \textbf{for} each slot $i \in \{1, \ldots, 30\}$ \textbf{do}
    \begin{enumerate}
        \item Extract features: $x_{ij^*} = [x_1, \ldots, x_{13}]$
        \item Predict cost: $\hat{y}_{ij^*} = M(x_{ij^*})$
    \end{enumerate}
    
    \item Filter feasible slots: $\mathcal{S}_{\text{feas}} = \{i : \text{Feasible}(i, j^*) = 1\}$
    
    \item Return: $i^* = \arg\min_{i \in \mathcal{S}_{\text{feas}}} \hat{y}_{ij^*}$
\end{enumerate}
\end{algorithm}

\subsection{Complexity Analysis}

Per-item prediction complexity:
\begin{equation}
\text{Time} = \mathcal{O}(30 \times 250) = \mathcal{O}(7500) \quad \text{(tree traversals)}
\end{equation}

\textbf{Practical runtime:} $< 1$ ms per item on modern CPU.

% ============================================================================
\section{Model Persistence}
% ============================================================================

\subsection{Serialization Format}

Models are stored using Python's \texttt{joblib} library:

\begin{equation}
M_{\text{saved}} = \{M_{\text{model}}, \mathcal{F}_{\text{cols}}\}
\end{equation}

where:
\begin{align*}
M_{\text{model}} &\in \text{HistGradientBoostingRegressor} \quad \text{(fitted estimator)} \\
\mathcal{F}_{\text{cols}} &\in \text{List}[\text{str}]^{13} \quad \text{(feature names for reproducibility)}
\end{align*}

\subsection{Storage Specifications}

\begin{table}[h]
\centering
\begin{tabular}{lc}
\toprule
\textbf{Property} & \textbf{Value} \\
\midrule
File format & Binary (joblib) \\
File size & $\sim 7$ MB \\
Location & \texttt{src/models/slotting\_model.joblib} \\
Load time & $< 100$ ms \\
\bottomrule
\end{tabular}
\end{table}

% ============================================================================
\section{Execution Workflow}
% ============================================================================

\subsection{End-to-End Pipeline}

\begin{equation}
\text{Streamlit UI} \rightarrow \text{Train} \rightarrow \text{Serialize} \rightarrow \text{Predict}
\end{equation}

\subsubsection{Step 1: Database Creation}

\begin{verbatim}
build_database()
├─ Create slots table (30 slots)
└─ Create items table (2000 items)
\end{verbatim}

\subsubsection{Step 2: Training Data Generation}

\begin{verbatim}
make_training_pairs()
├─ For each item: compute priority
├─ For each slot: compute oracle cost
└─ Generate 60K (features, label) pairs
\end{verbatim}

\subsubsection{Step 3: Model Training}

\begin{verbatim}
train_model(data)
├─ Split: 80% train, 20% test
├─ Fit: HistGradientBoosting (250 iterations)
├─ Evaluate: MAE ≈ 0.04
└─ Save: slotting_model.joblib
\end{verbatim}

\subsubsection{Step 4: Prediction}

\begin{verbatim}
predict_best_slot(item)
├─ Load model
├─ Generate features for all slots
├─ Predict costs
└─ Return best feasible slot
\end{verbatim}

\subsection{Timeline}

\begin{table}[h]
\centering
\begin{tabular}{lc}
\toprule
\textbf{Phase} & \textbf{Time} \\
\midrule
Database creation & $\sim 1$ s \\
Data pair generation & $\sim 30$ s \\
Model training (250 iter) & $\sim 45$ s \\
Total training time & $\sim 76$ s \\
Per-item prediction & $< 1$ ms \\
\bottomrule
\end{tabular}
\end{table}

% ============================================================================
\section{Mathematical Summary}
% ============================================================================

\subsection{Key Equations}

\begin{center}
\fbox{
\begin{minipage}{0.9\textwidth}
\textbf{Priority Score:}
\begin{equation*}
P = 0.6 \cdot \frac{d_m - 5}{115} + 0.4 \cdot p_o \quad P \in [0, 1]
\end{equation*}

\textbf{Oracle Cost Function:}
\begin{equation*}
\text{Cost}(i,j) = (0.2 + 0.6P) \cdot \frac{d_i - d_{\min}}{d_{\max} - d_{\min}} + (0.8 - 0.6P) \cdot \frac{v_w - v_{\min}}{v_{\max} - v_{\min}}
\end{equation*}

\textbf{Training Target:}
\begin{equation*}
y_{ij} = \begin{cases}
\text{Cost}(i,j) & \text{if Feasible}(i,j) = 1 \\
5.0 & \text{otherwise}
\end{cases}
\end{equation*}

\textbf{Prediction:}
\begin{equation*}
\hat{i} = \arg\min_{i: \text{Feasible}(i,j)=1} M_{\text{predict}}([x_1, \ldots, x_{13}])
\end{equation*}
\end{minipage}
}
\end{center}

\subsection{Notation Reference}

\begin{table}[h]
\centering
\begin{tabular}{ll}
\toprule
\textbf{Symbol} & \textbf{Meaning} \\
\midrule
$P$ & Priority score \\
$d_m$ & Monthly demand \\
$p_o$ & Order pressure \\
$d_i$ & Distance of slot $i$ \\
$L_m, W_m, H_m$ & Length, width, height (meters) \\
$w$ & Weight (kg) \\
$V_i, V_j$ & Slot volume, item volume \\
$\alpha(P)$ & Dynamic weighting factor \\
$\eta$ & Learning rate \\
$T$ & Number of boosting iterations \\
\bottomrule
\end{tabular}
\end{table}

% ============================================================================
\section{Conclusion}
% ============================================================================

The WarehouseMVP ML pipeline implements a sophisticated approach to warehouse optimization:

\begin{itemize}
    \item \textbf{Oracle-based learning}: Ground truth costs from physically-informed oracle function
    \item \textbf{Adaptive weighting}: Dynamic balance between proximity and space efficiency based on item demand
    \item \textbf{Scalable algorithm}: HistGradientBoosting handles 60K training samples efficiently
    \item \textbf{Production-ready}: Fast inference ($< 1$ ms/item) with $\approx 82\%$ accuracy
\end{itemize}

The system achieves strong performance (MAE $\approx 0.04$) while remaining computationally efficient and easily deployable via Streamlit.

\end{document}
